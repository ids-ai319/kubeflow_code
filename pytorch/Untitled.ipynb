{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "03ed44a2-dc9b-47b5-9d45-629cd6dda79f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets,transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ba921562-5583-49d8-8640-23eb136375fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 28\n",
    "num_classes = 10\n",
    "num_epochs =3\n",
    "batch_size =64\n",
    "\n",
    "train_dataset =datasets.MNIST(root = './',train = True, transform  = transforms.ToTensor(),download=True)\n",
    "test_dataset =datasets.MNIST(root = './',train = False, transform  = transforms.ToTensor(),download=True)\n",
    "\n",
    "train_loader =torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=True)\n",
    "test_loader =torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "                                         batch_size=batch_size,\n",
    "                                         shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "95295af5-3d51-48ba-873a-bb9f375d7184",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN,self).__init__()\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(1,16,kernel_size=5,stride=1,padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2)\n",
    "        )\n",
    "        self.conv2 =nn.Sequential(\n",
    "            nn.Conv2d(16,32,5,1,2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "        )\n",
    "        self.out = nn.Linear(32*7*7,10)\n",
    "    def forward(self,x):\n",
    "        x =self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = x.view(x.size(0),-1)\n",
    "        out = self.out(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "13c44bef-475d-4784-8d7e-bb14cb1e9f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def acc(predicitons,labels):\n",
    "    pred = torch.max(predicitons.data,1)[1]\n",
    "    rights =pred.eq(labels.data.view_as(pred)).sum()\n",
    "    return rights,len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c1fbc74e-5fd2-4fb3-a18a-8fee52b3b0f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "當前epoch: 1, loss: 0.0057, acc: 94.14%,花費時間:1.63秒\n",
      "當前epoch: 2, loss: 0.0710, acc: 98.17%,花費時間:1.66秒\n",
      "當前epoch: 3, loss: 0.0219, acc: 98.75%,花費時間:1.62秒\n",
      "當前epoch: 4, loss: 0.0031, acc: 98.93%,花費時間:1.71秒\n",
      "當前epoch: 5, loss: 0.0122, acc: 99.07%,花費時間:1.69秒\n",
      "當前epoch: 6, loss: 0.0049, acc: 99.31%,花費時間:1.65秒\n",
      "當前epoch: 7, loss: 0.0025, acc: 99.39%,花費時間:1.63秒\n",
      "當前epoch: 8, loss: 0.0007, acc: 99.51%,花費時間:1.63秒\n",
      "當前epoch: 9, loss: 0.0001, acc: 99.46%,花費時間:1.62秒\n",
      "當前epoch: 10, loss: 0.0027, acc: 99.68%,花費時間:1.68秒\n"
     ]
    }
   ],
   "source": [
    "model = CNN().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "num_epochs = 10  # Adjust this as needed\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    start_time =time.time()\n",
    "    train_right = []\n",
    "\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        model.train()\n",
    "        data,target = data.to(device),target.to(device)\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        right = acc(output, target)\n",
    "        train_right.append(right)\n",
    "\n",
    "    model.eval()\n",
    "    val_right = []\n",
    "\n",
    "    for (data, target) in test_loader:\n",
    "        data,target = data.to(device),target.to(device)\n",
    "        output = model(data)\n",
    "        right = acc(output, target)\n",
    "        val_right.append(right)\n",
    "\n",
    "    train_r = (sum([tup[0] for tup in train_right]), sum([tup[1] for tup in train_right]))\n",
    "    val_r = (sum([tup[0] for tup in val_right]), sum([tup[1] for tup in val_right]))\n",
    "    \n",
    "    end_time =time.time()\n",
    "    total =end_time-start_time\n",
    "    print(f'當前epoch: {epoch+1}, loss: {loss.item():.4f}, acc: {100 * train_r[0] / train_r[1]:.2f}%,花費時間:{total:.2f}秒')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "10d00d42-11b9-427b-abed-3eb849013d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "f = zipfile.ZipFile(\"./flowers.zip\",'r') # 原壓縮文件在服務器的位置\n",
    "for file in f.namelist():\n",
    "    f.extract(file,\"./\") #解壓到的位置，./表示當前目錄(與此.ipynb文件同一個目錄)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "539e6f9d-42cf-424b-ac1d-37b306d5e7e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torchvision import datasets,transforms\n",
    "import imageio,time,warnings,random,sys,copy\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "079ff3ca-2ace-430a-9dcd-0dae6f214bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = './flowers'\n",
    "train = data_dir+'/train'\n",
    "valid= data_dir+'/valid'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ccc6d43e-8dce-4ff6-9453-0b87597e3f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_transform = {\n",
    "    'train':transforms.Compose([transforms.RandomRotation(45),\n",
    "                        transforms.CenterCrop(224),\n",
    "                        transforms.RandomHorizontalFlip(p=0.5),\n",
    "                        transforms.RandomVerticalFlip(p=0.5),\n",
    "                        transforms.ColorJitter(brightness=0.2,contrast=0.1,saturation=0.1,hue=0.1),\n",
    "                        transforms.RandomGrayscale(p=0.025),\n",
    "                        transforms.ToTensor(),\n",
    "                        transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])]),\n",
    "    'valid':transforms.Compose([transforms.Resize(256),\n",
    "                        transforms.CenterCrop(224),\n",
    "                        transforms.ToTensor(),\n",
    "                        transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])])\n",
    "}  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "15cf9207-4d92-462c-b205-d35bd4fb5836",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "\n",
    "image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir,x),data_transform[x]) for x in ['train','valid']}\n",
    "dataloaders ={x:torch.utils.data.DataLoader(image_datasets[x],batch_size=batch_size,shuffle=True) for x in ['train','valid']}\n",
    "dataset_size = {x:len(image_datasets[x]) for x in ['train','valid']}\n",
    "class_numes = image_datasets['train'].classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5c459219-539b-49b9-b9f0-e9d1ed88f010",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': Dataset ImageFolder\n",
       "     Number of datapoints: 3252\n",
       "     Root location: ./flowers/train\n",
       "     StandardTransform\n",
       " Transform: Compose(\n",
       "                RandomRotation(degrees=[-45.0, 45.0], interpolation=nearest, expand=False, fill=0)\n",
       "                CenterCrop(size=(224, 224))\n",
       "                RandomHorizontalFlip(p=0.5)\n",
       "                RandomVerticalFlip(p=0.5)\n",
       "                ColorJitter(brightness=[0.8, 1.2], contrast=[0.9, 1.1], saturation=[0.9, 1.1], hue=[-0.1, 0.1])\n",
       "                RandomGrayscale(p=0.025)\n",
       "                ToTensor()\n",
       "                Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
       "            ),\n",
       " 'valid': Dataset ImageFolder\n",
       "     Number of datapoints: 1065\n",
       "     Root location: ./flowers/valid\n",
       "     StandardTransform\n",
       " Transform: Compose(\n",
       "                Resize(size=256, interpolation=bilinear)\n",
       "                CenterCrop(size=(224, 224))\n",
       "                ToTensor()\n",
       "                Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
       "            )}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e5ba7ece-e7c7-41f3-8655-23db4278e8fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': <torch.utils.data.dataloader.DataLoader at 0x7fbce9177eb0>,\n",
       " 'valid': <torch.utils.data.dataloader.DataLoader at 0x7fbce9177910>}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3e0a5a48-ea9f-4688-beff-9d6868251afa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': 3252, 'valid': 1065}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a2d5135b-da73-45d5-9d36-6fdd525adc2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1', '2', '3', '4', '5']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_numes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a50825a2-f398-4e90-bda8-0d5677a38325",
   "metadata": {},
   "outputs": [],
   "source": [
    "def im_convert(tensor):\n",
    "    \n",
    "    image = torch.tensor.to('cpu').clone.detach()\n",
    "    imgae = image.np().squeeze()\n",
    "    image = image.transpose(1,2,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4ee1fb31-7f60-4db7-bba7-01c0bfc28f96",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "imshow() missing 1 required positional argument: 'X'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[37], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(colums\u001b[38;5;241m*\u001b[39mrow):\n\u001b[1;32m      9\u001b[0m     ax \u001b[38;5;241m=\u001b[39m fig\u001b[38;5;241m.\u001b[39madd_subplot(row,colums,idx\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m,xticks\u001b[38;5;241m=\u001b[39m[],yticks\u001b[38;5;241m=\u001b[39m[])\n\u001b[0;32m---> 10\u001b[0m     \u001b[43mplt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimshow\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: imshow() missing 1 required positional argument: 'X'"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2000x1200 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure(figsize = (20,12))\n",
    "colums =4\n",
    "row=2\n",
    "\n",
    "dataiter = iter(dataloaders['valid'])\n",
    "input,classes = dataiter.next()\n",
    "\n",
    "for idx in range(colums*row):\n",
    "    ax = fig.add_subplot(row,colums,idx+1,xticks=[],yticks=[])\n",
    "    plt.imshow()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
